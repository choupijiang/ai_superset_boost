import os
import json
import base64
import logging
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Try to import OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("Warning: OpenAI not available")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ai_analyzer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def clean_ai_response(text: str) -> str:
    """Clean AI response by removing HTML tags, CSS styles, and other unwanted formatting"""
    if not text:
        return text
    
    import re
    
    # Step 1: Remove CSS style blocks and attributes completely
    # Remove style="..." attributes
    text = re.sub(r'style\s*=\s*"[^"]*"', '', text, flags=re.IGNORECASE)
    text = re.sub(r'style\s*=\s*\'[^\']*\'', '', text, flags=re.IGNORECASE)
    
    # Step 2: Remove CSS properties patterns that appear without style attributes
    css_patterns = [
        r'color\s*:\s*[^;]+;?',
        r'margin\s*:\s*[^;]+;?',
        r'padding\s*:\s*[^;]+;?',
        r'font-size\s*:\s*[^;]+;?',
        r'font-weight\s*:\s*[^;]+;?',
        r'background\s*:\s*[^;]+;?',
        r'border\s*:\s*[^;]+;?',
        r'text-align\s*:\s*[^;]+;?',
        r'line-height\s*:\s*[^;]+;?',
        r'font-family\s*:\s*[^;]+;?',
        r'display\s*:\s*[^;]+;?',
        r'width\s*:\s*[^;]+;?',
        r'height\s*:\s*[^;]+;?',
        r'list-style-type\s*:\s*[^;]+;?',
    ]
    
    for pattern in css_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # Step 3: Remove HTML tags completely
    text = re.sub(r'<[^>]*>', '', text)
    
    # Step 4: Remove any remaining HTML attributes
    text = re.sub(r'\w+\s*=\s*"[^"]*"', '', text)
    text = re.sub(r'\w+\s*=\s*\'[^\']*\'', '', text)
    
    # Step 5: Remove orphaned CSS values that might be left
    text = re.sub(r'#[0-9a-fA-F]{6}|#[0-9a-fA-F]{3}', '', text)  # Remove hex colors
    text = re.sub(r'\b\d+px\b', '', text)  # Remove pixel values
    text = re.sub(r'\b\d+em\b', '', text)  # Remove em values
    text = re.sub(r'\b\d+%\b', '', text)   # Remove percentage values
    
    # Step 6: Remove orphaned quotes and special characters
    text = re.sub(r'^["\'>\s]+', '', text, flags=re.MULTILINE)
    text = re.sub(r'["\'>\s]+$', '', text, flags=re.MULTILINE)
    
    # Step 7: Clean up multiple quotes and special characters
    text = re.sub(r'["\']{2,}', '', text)
    text = re.sub(r'["\'>]', '', text)
    
    # Step 8: Split by lines and clean each line
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Remove any remaining CSS-like patterns more aggressively
        line = re.sub(r'\b\w+\s*:\s*[^;]*;?', '', line, flags=re.IGNORECASE)
        
        # Remove orphaned CSS property names and values
        line = re.sub(r'\b(border|padding|margin|color|font|background|width|height|display|text-align|line-height|font-weight|font-size)\s*[-\w]*', '', line, flags=re.IGNORECASE)
        
        # Clean up spacing
        line = re.sub(r'\s+', ' ', line)
        line = line.replace(' .', '.').replace(' ,', ',').replace(' ;', ';')
        
        if line.strip():
            cleaned_lines.append(line.strip())
    
    # Step 9: Join cleaned lines and ensure proper markdown formatting
    text = '\n\n'.join(cleaned_lines)
    
    # Step 10: Final cleanup - remove any lines that are just CSS properties or orphaned CSS parts
    lines = text.split('\n')
    final_lines = []
    for line in lines:
        line = line.strip()
        # Skip lines that look like CSS properties or contain only CSS-related words
        if not re.match(r'^[a-zA-Z-]+\s*:\s*.*;?$', line) and not re.match(r'^\s*(border|padding|margin|color|font|background)\s*[-\w]*\s*$', line, flags=re.IGNORECASE):
            final_lines.append(line)
    
    text = '\n'.join(final_lines)
    
    return text.strip()

class AIAnalyzer:
    def __init__(self):
        self.openai_api_key = os.environ.get('OPENAI_API_KEY')
        self.openai_api_base = os.environ.get('OPENAI_API_BASE')
        self.openai_model = os.environ.get('OPENAI_MODEL', 'glm-4v-plus')
        
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI library is required but not available")
        
        self.client = OpenAI(
            api_key=self.openai_api_key,
            base_url=self.openai_api_base
        )
        logger.info(f"âœ… Using GLM-4.5 with BigModel.cn - Model: {self.openai_model}")
    
    def encode_image(self, image_path: str) -> Optional[str]:
        """Encode image to base64 with proper data URL format for BigModel.cn"""
        try:
            with open(image_path, "rb") as image_file:
                image_data = image_file.read()
                base64_data = base64.b64encode(image_data).decode('utf-8')
                
                # Determine image type from file extension
                import mimetypes
                mime_type, _ = mimetypes.guess_type(image_path)
                if not mime_type:
                    mime_type = 'image/png'
                
                # Format as data URL: data:image/png;base64,<base64_data>
                data_url = f"data:{mime_type};base64,{base64_data}"
                return data_url
        except Exception as e:
            logger.error(f"Error encoding image {image_path}: {e}")
            return None
    
    def _call_ai_api(self, messages: List[Dict], model: str = None, max_tokens: int = 2000, timeout: float = 30.0) -> str:
        """Generic AI API call method"""
        try:
            # Configure timeout and retry settings
            response = self.client.chat.completions.create(
                model=model or self.openai_model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=0.3,
                timeout=timeout  # Configurable timeout
            )
            content = response.choices[0].message.content
            if not content or content.strip() == "":
                logger.warning("âš ï¸ AI API returned empty content")
                logger.warning(f"âš ï¸ Response details: {response}")
                logger.warning(f"âš ï¸ Model used: {model or self.openai_model}")
                logger.warning(f"âš ï¸ Max tokens: {max_tokens}")
                return "AI APIè¿”å›ç©ºå†…å®¹ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé™åˆ¶"
            return content
        except Exception as e:
            error_msg = f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            # Check if it's a connection error and provide more helpful message
            if "Connection error" in str(e) or "timeout" in str(e).lower():
                logger.warning("âš ï¸ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
            return error_msg
    
    def analyze_dashboard_progressively(self, question: str, dashboard_data: Dict[str, Any], progress_callback=None) -> str:
        """
        Analyze a single dashboard progressively with simplified approach
        
        Args:
            question: Business question to analyze
            dashboard_data: Dictionary containing dashboard information and screenshots
            progress_callback: Optional callback function to send progress updates
        """
        try:
            dashboard_title = dashboard_data.get('dashboard_title', 'Unknown')
            logger.info(f"ğŸ¤– Analyzing dashboard: {dashboard_title}")
            
            # Send start event if callback provided
            if progress_callback:
                progress_callback({
                    'type': 'analysis_started',
                    'dashboard_title': dashboard_title,
                    'message': 'å¼€å§‹åˆ†æçœ‹æ¿æ•°æ®'
                })
            
            # Get dashboard screenshot
            dashboard_screenshot = dashboard_data.get('dashboard_screenshot')
            if not dashboard_screenshot:
                return f"çœ‹æ¿ {dashboard_title} æ²¡æœ‰å¯ç”¨çš„æˆªå›¾"
            
            # Construct full path if needed
            if not os.path.isabs(dashboard_screenshot):
                # Convert from relative URL path to filesystem path
                if dashboard_screenshot.startswith('screenshots/'):
                    dashboard_screenshot = os.path.join(os.path.dirname(__file__), dashboard_screenshot)
                else:
                    dashboard_screenshot = os.path.join('screenshots', dashboard_screenshot)
            
            if not os.path.exists(dashboard_screenshot):
                return f"çœ‹æ¿æˆªå›¾æ–‡ä»¶ä¸å­˜åœ¨: {dashboard_screenshot}"
            
            # Encode image
            image_base64 = self.encode_image(dashboard_screenshot)
            if not image_base64:
                return f"å›¾ç‰‡ç¼–ç å¤±è´¥: {dashboard_screenshot}"
            
            # Prepare content with just question and image
            content = [
                {"type": "text", "text": f"è¯·åˆ†æè¿™ä¸ªçœ‹æ¿æˆªå›¾ï¼Œå›ç­”ä¸šåŠ¡é—®é¢˜ï¼š{question}"},
                {"type": "image_url", "image_url": {"url": image_base64}}
            ]
            
            # Simple system message
            system_message = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šæ•°æ®åˆ†æå¸ˆã€‚è¯·ä»”ç»†åˆ†æçœ‹æ¿æˆªå›¾ï¼Œè¯†åˆ«å…³é”®æŒ‡æ ‡å’Œè¶‹åŠ¿ï¼Œå›ç­”ç”¨æˆ·çš„ä¸šåŠ¡é—®é¢˜ã€‚

æå…¶ä¸¥æ ¼çš„è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæä¾›å…·ä½“çš„æ´å¯Ÿå’Œå»ºè®®
2. ç»å¯¹ç¦æ­¢ä½¿ç”¨ä»»ä½•HTMLæ ‡ç­¾ã€CSSæ ·å¼ã€Markdownæ ¼å¼æˆ–å…¶ä»–æ ¼å¼åŒ–ä»£ç 
3. åªè¿”å›çº¯æ–‡æœ¬å†…å®¹ï¼Œä¸¥ç¦ä»»ä½•æ ¼å¼åŒ–
4. ä¸¥ç¦è¿”å›ä»»ä½•åŒ…å«ä»¥ä¸‹å†…å®¹çš„å†…å®¹ï¼š
   - ä»»ä½•HTMLæ ‡ç­¾ï¼ˆå¦‚ <div>, <span>, <p>, <h1> ç­‰ï¼‰
   - ä»»ä½•CSSæ ·å¼ï¼ˆå¦‚ style="color: #2c3e50; margin: 20px 0 10px 0;"ï¼‰
   - ä»»ä½•CSSå±æ€§ï¼ˆå¦‚ color: #2c3e50; margin: 20px 0 10px 0;ï¼‰
   - ä»»ä½•é¢œè‰²ä»£ç ï¼ˆå¦‚ #2c3e50, #667eea ç­‰ï¼‰
   - ä»»ä½•å­—ä½“å¤§å°ï¼ˆå¦‚ 1.3em, 14px ç­‰ï¼‰
   - ä»»ä½•è¾¹æ¡†æˆ–å¸ƒå±€å±æ€§ï¼ˆå¦‚ border-left: 3px solid #667eea;ï¼‰
   - ä»»ä½•JavaScriptä»£ç 
   - ä»»ä½•ç¼–ç¨‹ä»£ç æˆ–æ ¼å¼åŒ–æ ‡è®°
   - ä»»ä½•å¼•å·åŒ…å«çš„æ ·å¼ä¿¡æ¯
   - ä»»ä½•åˆ—è¡¨æ ·å¼ï¼ˆå¦‚ list-style-type: disc;ï¼‰

5. è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹çº¯æ–‡æœ¬æ ¼å¼å›ç­”ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ ¼å¼åŒ–ï¼š

ä¸»è¦å‘ç°ï¼š
[åœ¨è¿™é‡Œæè¿°ä»çœ‹æ¿ä¸­è§‚å¯Ÿåˆ°çš„ä¸»è¦æ•°æ®å’Œè¶‹åŠ¿ï¼Œä½¿ç”¨çº¯æ–‡æœ¬]

å…³é”®æ´å¯Ÿï¼š
[åœ¨è¿™é‡Œæä¾›2-3ä¸ªæœ€é‡è¦çš„ä¸šåŠ¡æ´å¯Ÿï¼Œä½¿ç”¨çº¯æ–‡æœ¬]

å»ºè®®ï¼š
[åœ¨è¿™é‡ŒåŸºäºæ•°æ®æä¾›å…·ä½“çš„ä¸šåŠ¡å»ºè®®ï¼Œä½¿ç”¨çº¯æ–‡æœ¬]

6. å¦‚æœè¿åä»¥ä¸Šä»»ä½•è¦æ±‚ï¼Œå°†ä¼šä¸¥é‡å½±å“ç”¨æˆ·ä½“éªŒï¼Œè¯·åŠ¡å¿…åªè¿”å›çº¯æ–‡æœ¬å†…å®¹ã€‚
7. ä¸è¦æ·»åŠ ä»»ä½•é¢œè‰²ã€å­—ä½“ã€è¾¹æ¡†ã€é—´è·ç­‰CSSç›¸å…³çš„æè¿°ã€‚
8. å†…å®¹åº”è¯¥åƒçº¯æ–‡æœ¬æ–‡æ¡£ä¸€æ ·ç®€æ´æ˜äº†ã€‚"""
            
            # Call AI API
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": content}
            ]
            
            result = self._call_ai_api(messages)
            if result.startswith("AI APIè°ƒç”¨å¤±è´¥") or result.startswith("AI APIè¿”å›ç©ºå†…å®¹"):
                return result
            
            # Log the original AI response for debugging
            logger.info(f"ğŸ” Original AI response for {dashboard_title}:")
            logger.info(f"--- BEGIN RAW AI RESPONSE ---")
            logger.info(result)
            logger.info(f"--- END RAW AI RESPONSE ---")
            
            # Clean the result to remove any HTML tags or unwanted formatting
            cleaned_result = clean_ai_response(result)
            
            # Log the cleaned result for comparison
            logger.info(f"ğŸ§¹ Cleaned AI response for {dashboard_title}:")
            logger.info(f"--- BEGIN CLEANED AI RESPONSE ---")
            logger.info(cleaned_result)
            logger.info(f"--- END CLEANED AI RESPONSE ---")
            
            result = cleaned_result
            
            logger.info(f"âœ… Dashboard analysis completed: {dashboard_title}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Dashboard analysis failed: {e}")
            return f"åˆ†æçœ‹æ¿ {dashboard_data.get('dashboard_title', 'Unknown')} æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    def combine_multiple_analyses(self, question: str, individual_analyses: List[Dict[str, Any]]) -> str:
        """
        Combine multiple individual dashboard analyses into a comprehensive answer
        
        Args:
            question: Original business question
            individual_analyses: List of dictionaries containing dashboard analysis results
        """
        try:
            logger.info(f"ğŸ¤– Combining {len(individual_analyses)} analyses")
            
            # Prepare analysis summaries
            analysis_summaries = []
            for i, analysis in enumerate(individual_analyses):
                dashboard_title = analysis.get('dashboard_title', f'çœ‹æ¿{i+1}')
                analysis_result = analysis.get('analysis', 'æ— åˆ†æç»“æœ')
                analysis_summaries.append(f"çœ‹æ¿ {i+1}: {dashboard_title}\nåˆ†æç»“æœ: {analysis_result}")
            
            combined_text = "\n\n".join(analysis_summaries)
            
            # Simple prompt for combining analyses
            prompt = f"""åŸå§‹ä¸šåŠ¡é—®é¢˜ï¼š{question}

å„çœ‹æ¿çš„ç‹¬ç«‹åˆ†æç»“æœï¼š
{combined_text}

è¯·ç»¼åˆä»¥ä¸Šæ‰€æœ‰çœ‹æ¿çš„åˆ†æç»“æœï¼Œæä¾›ä¸€ä¸ªå…¨é¢çš„å›ç­”ã€‚

æå…¶ä¸¥æ ¼çš„æ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š

æ•´ä½“æ´å¯Ÿï¼š
ç»¼åˆæ‰€æœ‰çœ‹æ¿æ•°æ®çš„å…³é”®å‘ç°

å…³é”®è¶‹åŠ¿ï¼š
è¯†åˆ«æœ€é‡è¦çš„è¶‹åŠ¿å’Œæ¨¡å¼

è·¨çœ‹æ¿å…³è”ï¼š
åˆ†æä¸åŒçœ‹æ¿æ•°æ®ä¹‹é—´çš„å…³ç³»

ç»¼åˆå»ºè®®ï¼š
æä¾›æ•´ä½“æ€§çš„ä¸šåŠ¡å»ºè®®

ç»å¯¹ä¸¥æ ¼ç¦æ­¢ï¼š
- ä¸¥ç¦ä½¿ç”¨ä»»ä½•HTMLæ ‡ç­¾ï¼ˆå¦‚ <div>, <span>, <p>, <h1> ç­‰ï¼‰
- ä¸¥ç¦ä½¿ç”¨CSSæ ·å¼ï¼ˆå¦‚ style="color: #2c3e50; margin: 20px 0 10px 0;"ï¼‰
- ä¸¥ç¦ä½¿ç”¨CSSå±æ€§ï¼ˆå¦‚ color: #2c3e50; margin: 20px 0 10px 0;ï¼‰
- ä¸¥ç¦ä½¿ç”¨é¢œè‰²ä»£ç ï¼ˆå¦‚ #2c3e50, #667eea ç­‰ï¼‰
- ä¸¥ç¦ä½¿ç”¨å­—ä½“å¤§å°ï¼ˆå¦‚ 1.3em, 14px ç­‰ï¼‰
- ä¸¥ç¦ä½¿ç”¨è¾¹æ¡†æˆ–å¸ƒå±€å±æ€§ï¼ˆå¦‚ border-left: 3px solid #667eea;ï¼‰
- ä¸¥ç¦ä½¿ç”¨åˆ—è¡¨æ ·å¼ï¼ˆå¦‚ list-style-type: disc;ï¼‰
- ä¸¥ç¦ä½¿ç”¨Markdownæ ¼å¼
- ä¸¥ç¦ä½¿ç”¨ä»»ä½•æ ¼å¼åŒ–ä»£ç æˆ–æ ‡è®°
- ä¸¥ç¦ä½¿ç”¨ä»»ä½•å¼•å·åŒ…å«çš„æ ·å¼ä¿¡æ¯

è¯·åªè¿”å›çº¯æ–‡æœ¬å†…å®¹ï¼Œä½¿ç”¨æ¸…æ™°çš„æ®µè½å’Œç®€å•çš„æ ‡é¢˜ã€‚å¦‚æœè¿”å›ä»»ä½•æ ¼å¼åŒ–ä»£ç å°†ä¸¥é‡å½±å“ç”¨æˆ·ä½“éªŒã€‚
å†…å®¹åº”è¯¥åƒçº¯æ–‡æœ¬æ–‡æ¡£ä¸€æ ·ç®€æ´æ˜äº†ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§†è§‰æ ·å¼çš„æè¿°ã€‚"""
            
            # Call AI API for summary
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šæ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ç»¼åˆå¤šæºæ•°æ®è¿›è¡Œå…¨é¢åˆ†æã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸¥ç¦ä½¿ç”¨ä»»ä½•HTMLæ ‡ç­¾ã€CSSæ ·å¼æˆ–æ ¼å¼åŒ–ä»£ç ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            result = self._call_ai_api(messages, model='glm-4-plus', max_tokens=3000)
            if result.startswith("AI APIè°ƒç”¨å¤±è´¥") or result.startswith("AI APIè¿”å›ç©ºå†…å®¹"):
                return result
            
            # Log the original AI response for debugging
            logger.info(f"ğŸ” Original combined AI response:")
            logger.info(f"--- BEGIN RAW COMBINED AI RESPONSE ---")
            logger.info(result)
            logger.info(f"--- END RAW COMBINED AI RESPONSE ---")
            
            # Clean the result to remove any HTML tags or unwanted formatting
            cleaned_result = clean_ai_response(result)
            
            # Log the cleaned result for comparison
            logger.info(f"ğŸ§¹ Cleaned combined AI response:")
            logger.info(f"--- BEGIN CLEANED COMBINED AI RESPONSE ---")
            logger.info(cleaned_result)
            logger.info(f"--- END CLEANED COMBINED AI RESPONSE ---")
            
            result = cleaned_result
            
            logger.info(f"âœ… Successfully combined {len(individual_analyses)} analyses")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Failed to combine analyses: {e}")
            return f"ç»¼åˆåˆ†ææ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
    
    # Legacy methods for backward compatibility
    def analyze_multimodal(self, question: str, screenshots: Optional[List[Dict[str, Any]]] = None, json_data: Optional[Union[Dict, List]] = None) -> str:
        """Legacy method - use simplified dashboard analysis instead"""
        if screenshots:
            # Convert to dashboard format and use new method
            dashboard_data = {
                'dashboard_title': 'Legacy Analysis',
                'dashboard_screenshot': screenshots[0]['path']
            }
            return self.analyze_dashboard_progressively(question, dashboard_data)
        return self._get_fallback_response(question)
    
    def analyze_with_screenshots(self, question: str, screenshots: List[Dict[str, Any]]) -> str:
        """Legacy method"""
        return self.analyze_multimodal(question, screenshots=screenshots)
    
    def analyze_with_json(self, question: str, json_data: Union[Dict, List]) -> str:
        """Legacy method"""
        return self._get_fallback_response(question)
    
    def analyze_text_only(self, question: str, dashboard_titles: List[str]) -> str:
        """Legacy method"""
        return self._get_fallback_response(question)
    
    def _get_fallback_response(self, question: str) -> str:
        """Get fallback response when AI analysis fails"""
        return f"""
æŠ±æ­‰ï¼ŒAI åˆ†ææš‚æ—¶æ— æ³•å®Œæˆã€‚å¯¹äºæ‚¨çš„ä¸šåŠ¡é—®é¢˜"{question}"ï¼Œå»ºè®®æ‚¨ï¼š

1. ç›´æ¥æŸ¥çœ‹ Superset ä»ªè¡¨æ¿è·å–è¯¦ç»†æ•°æ®
2. è”ç³»æŠ€æœ¯æ”¯æŒæ£€æŸ¥ AI æœåŠ¡é…ç½®
3. ç¨åé‡è¯•åˆ†æ

å½“å‰ä½¿ç”¨çš„ AI æä¾›å•†: GLM-4.5 (BigModel.cn)
è¯·æ£€æŸ¥ API å¯†é’¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚
"""