from flask import Flask, render_template, request, jsonify, send_from_directory, Response
import os
import asyncio
import json
import logging
from datetime import datetime
from superset_automation import SupersetAutomation
from ai_analyzer import AIAnalyzer
from context_manager import SmartContextSystem
from queue import Queue
import threading
import time

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'dev-secret-key')

# Configure logging - Comprehensive logging as described in README.md
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡å­˜å‚¨æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿ
smart_context_system = None

def initialize_system():
    """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶ï¼ŒåŒ…æ‹¬FAISSç´¢å¼•"""
    global smart_context_system
    
    try:
        logger.info("ğŸš€ åˆå§‹åŒ–æ™ºèƒ½å•†ä¸šåˆ†æç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–AIåˆ†æå™¨
        ai_analyzer = AIAnalyzer()
        
        # åˆå§‹åŒ–æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿ
        smart_context_system = SmartContextSystem(ai_analyzer, use_faiss=True)
        
        # æ£€æŸ¥FAISSç´¢å¼•çŠ¶æ€
        if smart_context_system.use_faiss and smart_context_system.faiss_index_manager:
            logger.info("ğŸ” æ£€æŸ¥FAISSç´¢å¼•çŠ¶æ€...")
            
            # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
            if not smart_context_system.faiss_index_manager.load_existing_index():
                logger.info("ğŸ“ æœªæ‰¾åˆ°ç°æœ‰FAISSç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°ç´¢å¼•...")
                
                # ä»contextä¸‹çš„markdownæ–‡ä»¶æ„å»ºFAISSç´¢å¼•
                if smart_context_system.faiss_index_manager.build_index_from_contexts():
                    logger.info("âœ… FAISSç´¢å¼•æ„å»ºæˆåŠŸ")
                else:
                    logger.warning("âš ï¸ FAISSç´¢å¼•æ„å»ºå¤±è´¥ï¼Œå°†ä½¿ç”¨AIé€‰æ‹©ä½œä¸ºå¤‡é€‰")
            else:
                logger.info("âœ… FAISSç´¢å¼•åŠ è½½æˆåŠŸ")
        
        logger.info("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def initialize_system_on_first_request():
    """åœ¨ç¬¬ä¸€ä¸ªè¯·æ±‚ä¹‹å‰åˆå§‹åŒ–ç³»ç»Ÿ"""
    if not hasattr(app, '_system_initialized'):
        logger.info("ğŸ“± æ”¶åˆ°ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œåˆå§‹åŒ–ç³»ç»Ÿ...")
        initialize_system()
        app._system_initialized = True

# åœ¨ä¸»è¯·æ±‚å¤„ç†ä¸­è°ƒç”¨åˆå§‹åŒ–
@app.before_request
def before_request():
    """åœ¨æ¯ä¸ªè¯·æ±‚ä¹‹å‰æ£€æŸ¥ç³»ç»Ÿåˆå§‹åŒ–çŠ¶æ€"""
    initialize_system_on_first_request()

def get_screenshot_url(screenshot_path):
    """Convert full file path to relative URL path for web access"""
    if not screenshot_path:
        return None
    
    # If it's already a relative path, return as is
    if screenshot_path.startswith('screenshots/'):
        return screenshot_path
    
    # Extract just the filename from the full path
    filename = os.path.basename(screenshot_path)
    return f'screenshots/{filename}'

def ensure_full_dashboard_url(url):
    """Ensure dashboard URL is a full URL for proper linking"""
    if not url:
        return '#'
    
    # If it's already a full URL, return as is
    if url.startswith('http://') or url.startswith('https://'):
        return url
    
    # Get Superset URL from environment
    superset_url = os.environ.get('SUPERSET_URL', 'http://localhost:8088')
    
    # Remove trailing slash from superset_url if present
    superset_url = superset_url.rstrip('/')
    
    # Handle relative URLs
    if url.startswith('/'):
        return f"{superset_url}{url}"
    else:
        return f"{superset_url}/{url}"

@app.route('/')
def index():
    logger.info("ğŸ“± ç”¨æˆ·è®¿é—®é¦–é¡µ")
    return render_template('index.html')

@app.route('/context-status')
def context_status():
    """Get context system status"""
    try:
        # ä½¿ç”¨å…¨å±€çš„smart_context_system
        global smart_context_system
        
        if smart_context_system is None:
            logger.warning("âš ï¸ æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            initialize_system()
        
        if smart_context_system is None:
            return jsonify({
                "error": "æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥",
                "faiss_enabled": False,
                "selection_method": "æœªåˆå§‹åŒ–"
            }), 500
        
        status = smart_context_system.get_system_status()
        
        # Add additional helpful information
        contexts = smart_context.context_manager.get_all_contexts()
        context_details = []
        
        for context in contexts:
            context_details.append({
                'dashboard_id': context.dashboard_id,
                'dashboard_name': context.dashboard_name,
                'last_update_time': context.last_update_time,
                'is_expired': context.is_expired(),
                'charts_count': len(context.charts),
                'file_path': context.file_path
            })
        
        enhanced_status = {
            **status,
            'context_details': context_details,
            'server_time': datetime.now().isoformat(),
            'update_frequency_days': smart_context.context_manager.update_frequency_days
        }
        
        return jsonify(enhanced_status)
    except Exception as e:
        logger.error(f"âŒ Failed to get context status: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/context-refresh', methods=['POST'])
def context_refresh():
    """Manually refresh context system"""
    try:
        logger.info("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°Contextç³»ç»Ÿ...")
        
        # ä½¿ç”¨å…¨å±€çš„smart_context_system
        global smart_context_system
        
        if smart_context_system is None:
            logger.warning("âš ï¸ æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            initialize_system()
        
        if smart_context_system is None:
            return jsonify({
                'success': False,
                'error': 'æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥',
                'timestamp': datetime.now().isoformat()
            }), 500
        
        smart_context = smart_context_system
        
        # Get available dashboards and update contexts
        async def refresh_contexts():
            try:
                async with SupersetAutomation() as superset_automation:
                    dashboard_list = await superset_automation.get_dashboard_list()
                    
                    if dashboard_list:
                        # Step 1: Capture screenshots for all dashboards first
                        logger.info("ğŸ“¸ Capturing screenshots for all dashboards...")
                        available_dashboards = []
                        
                        for i, dashboard in enumerate(dashboard_list, 1):
                            logger.info(f"ğŸ”„ Processing {i}/{len(dashboard_list)}: {dashboard.get('title')}")
                            
                            try:
                                # Capture dashboard screenshot
                                screenshot_path = await superset_automation.capture_dashboard_screenshot(dashboard)
                                
                                # Create simplified dashboard data
                                simplified_dashboard = {
                                    'dashboard_id': str(dashboard.get('id', '')),
                                    'dashboard_title': dashboard.get('title', ''),
                                    'dashboard_url': dashboard.get('url', ''),
                                    'published': dashboard.get('published', False),
                                    'changed_on': dashboard.get('changed_on', ''),
                                    'dashboard_screenshot': get_screenshot_url(screenshot_path),
                                    'charts': []
                                }
                                available_dashboards.append(simplified_dashboard)
                                
                                if screenshot_path:
                                    logger.info(f"âœ… Screenshot captured: {screenshot_path}")
                                else:
                                    logger.warning(f"âš ï¸ Failed to capture screenshot for: {dashboard.get('title')}")
                                    
                            except Exception as e:
                                logger.error(f"âŒ Error processing dashboard {dashboard.get('title')}: {e}")
                                # Still add to available_dashboards but without screenshot
                                simplified_dashboard = {
                                    'dashboard_id': str(dashboard.get('id', '')),
                                    'dashboard_title': dashboard.get('title', ''),
                                    'dashboard_url': dashboard.get('url', ''),
                                    'published': dashboard.get('published', False),
                                    'changed_on': dashboard.get('changed_on', ''),
                                    'dashboard_screenshot': None,
                                    'charts': []
                                }
                                available_dashboards.append(simplified_dashboard)
                            
                            # Add delay between processing to avoid overwhelming the system
                            if i < len(dashboard_list):
                                await asyncio.sleep(1)
                        
                        logger.info(f"âœ… Completed processing {len(available_dashboards)} dashboards")
                        
                        # Step 2: Update contexts with screenshots (force update all)
                        logger.info("ğŸ¤– Analyzing dashboard content with AI...")
                        update_results = smart_context.update_dashboard_contexts(available_dashboards, force_update=True)
                        return update_results
                    else:
                        return {'error': 'No dashboards found'}
            except Exception as e:
                return {'error': str(e)}
        
        # Run async refresh
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            update_results = loop.run_until_complete(refresh_contexts())
        finally:
            loop.close()
        
        logger.info("âœ… Contextç³»ç»Ÿåˆ·æ–°å®Œæˆ")
        
        # å¼ºåˆ¶é‡å»ºFAISSç´¢å¼•
        if smart_context.use_faiss and smart_context.faiss_index_manager:
            logger.info("ğŸ”„ é‡å»ºFAISSç´¢å¼•...")
            try:
                if smart_context.faiss_index_manager.force_rebuild():
                    logger.info("âœ… FAISSç´¢å¼•é‡å»ºæˆåŠŸ")
                else:
                    logger.warning("âš ï¸ FAISSç´¢å¼•é‡å»ºå¤±è´¥")
            except Exception as e:
                logger.error(f"âŒ FAISSç´¢å¼•é‡å»ºé”™è¯¯: {e}")
        
        return jsonify({
            'success': True,
            'message': 'Context system refreshed successfully',
            'update_results': update_results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ Contextåˆ·æ–°å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

def run_async_analysis(question):
    """Run async analysis in a separate thread"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        return loop.run_until_complete(analyze_question_async(question))
    finally:
        loop.close()

async def analyze_question_async(question):
    """Async analysis function with smart context system - optimized version"""
    # ä½¿ç”¨å…¨å±€çš„smart_context_system
    global smart_context_system
    
    if smart_context_system is None:
        logger.warning("âš ï¸ æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨é‡æ–°åˆå§‹åŒ–...")
        initialize_system()
        if smart_context_system is None:
            logger.error("âŒ æ™ºèƒ½ä¸Šä¸‹æ–‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return {
                'question': question,
                'answer': 'ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚',
                'timestamp': datetime.now().isoformat(),
                'analysis_type': 'system_error',
                'dashboards_analyzed': 0,
                'total_charts': 0,
                'dashboards_data': [],
                'individual_analyses': [],
                'screenshots': []
            }
    
    smart_context = smart_context_system
    ai_analyzer = smart_context.dashboard_analyzer.ai_analyzer
    
    async with SupersetAutomation() as superset_automation:
        
        try:
            logger.info(f"ğŸ” å¼€å§‹æ™ºèƒ½åˆ†æ: {question[:100]}...")
            
            # Step 1: Get available dashboards (like test_download_all_dashboards.py)
            logger.info("ğŸ“‹ è·å–Dashboardåˆ—è¡¨...")
            dashboard_list = await superset_automation.get_dashboard_list()
            
            if not dashboard_list:
                logger.warning("âš ï¸ æ— æ³•è·å–çœ‹æ¿åˆ—è¡¨")
                return {
                    'question': question,
                    'answer': 'æŠ±æ­‰ï¼Œç›®å‰æ— æ³•è¿æ¥åˆ°Supersetæˆ–è·å–çœ‹æ¿åˆ—è¡¨ã€‚\n\nå¯èƒ½çš„åŸå› ï¼š\n1. SupersetæœåŠ¡æœªè¿è¡Œï¼ˆè¯·æ£€æŸ¥ http://localhost:8088ï¼‰\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. ç™»å½•å‡­æ®é”™è¯¯\n4. Superseté…ç½®é—®é¢˜\n\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n- ç¡®è®¤SupersetæœåŠ¡å·²å¯åŠ¨\n- æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®ï¼ˆ.envæ–‡ä»¶ï¼‰\n- éªŒè¯SUPERSET_URLã€SUPERSET_USERNAMEã€SUPERSET_PASSWORDæ˜¯å¦æ­£ç¡®\n- ç¨åé‡è¯•',
                    'timestamp': datetime.now().isoformat(),
                    'analysis_type': 'no_data',
                    'dashboards_analyzed': 0,
                    'total_charts': 0,
                    'dashboards_data': [],
                    'individual_analyses': [],
                    'screenshots': []
                }
            
            logger.info(f"âœ… æ‰¾åˆ° {len(dashboard_list)} ä¸ªDashboards")
            
            # Step 2: Convert to simplified format and update contexts
            available_dashboards = []
            for dashboard in dashboard_list:
                simplified_dashboard = {
                    'dashboard_id': str(dashboard.get('id', '')),
                    'dashboard_title': dashboard.get('title', ''),
                    'dashboard_url': dashboard.get('url', ''),
                    'published': dashboard.get('published', False),
                    'changed_on': dashboard.get('changed_on', ''),
                    'dashboard_screenshot': None,
                    'charts': []
                }
                available_dashboards.append(simplified_dashboard)
            
            # Step 3: Update contexts for expired dashboards
            logger.info("ğŸ”„ æ›´æ–°Dashboard Contexts...")
            update_results = smart_context.update_dashboard_contexts(available_dashboards)
            
            # Check if update_results contains an error
            if 'error' in update_results:
                logger.error(f"âŒ Context update failed: {update_results['error']}")
                updated_count = 0
                expired_count = 0
            else:
                updated_count = len(update_results.get('updated_contexts', []))
                expired_count = len(update_results.get('expired_dashboards', []))
                logger.info(f"ğŸ“Š Contextæ›´æ–°: æ–°å¢{updated_count}ä¸ª, è¿‡æœŸ{expired_count}ä¸ª")
            
            # Step 4: Select most relevant dashboards using FAISS or AI
            logger.info("ğŸ¯ æ™ºèƒ½é€‰æ‹©ç›¸å…³Dashboards...")
            if smart_context.use_faiss and smart_context.faiss_index_manager:
                logger.info("ğŸ” ä½¿ç”¨FAISSå‘é‡æœç´¢è¿›è¡ŒDashboardé€‰æ‹©...")
            else:
                logger.info("ğŸ¤– ä½¿ç”¨AIè¿›è¡ŒDashboardé€‰æ‹©...")
            
            selected_dashboards = smart_context.select_dashboards_for_question(question, top_k=3)
            
            if not selected_dashboards:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„Dashboards")
                return {
                    'question': question,
                    'answer': 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„çœ‹æ¿ã€‚\n\nå¯èƒ½çš„åŸå› ï¼š\n1. å½“å‰çœ‹æ¿å†…å®¹ä¸æ‚¨çš„é—®é¢˜ä¸åŒ¹é…\n2. çœ‹æ¿æ•°æ®å¯èƒ½éœ€è¦æ›´æ–°\n3. é—®é¢˜è¡¨è¿°å¯èƒ½éœ€è¦è°ƒæ•´\n\nå»ºè®®ï¼š\n- å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜\n- æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„çœ‹æ¿å­˜åœ¨\n- è”ç³»ç®¡ç†å‘˜ç¡®è®¤çœ‹æ¿å†…å®¹',
                    'timestamp': datetime.now().isoformat(),
                    'analysis_type': 'no_relevant_dashboards',
                    'dashboards_analyzed': 0,
                    'total_charts': 0,
                    'dashboards_data': [],
                    'individual_analyses': [],
                    'screenshots': []
                }
            
            logger.info(f"âœ… é€‰æ‹©äº† {len(selected_dashboards)} ä¸ªç›¸å…³Dashboards")
            for i, (context, score) in enumerate(selected_dashboards):
                logger.info(f"   {i+1}. {context.dashboard_name} (ç›¸å…³åº¦: {score:.2f})")
            
            # Step 5: Progressive analysis of selected dashboards (like test_download_all_dashboards.py)
            logger.info("ğŸš€ å¼€å§‹æ¸è¿›å¼åˆ†æ...")
            
            individual_analyses = []
            dashboards_data = []
            selected_dashboard_ids = [ctx.dashboard_id for ctx, score in selected_dashboards]
            
            # Define callback function for progressive analysis
            async def analyze_dashboard_callback(dashboard_data, dashboard_index, total_dashboards):
                """Callback function to analyze each dashboard immediately after capture"""
                try:
                    dashboard_id = dashboard_data.get('dashboard_id')
                    dashboard_title = dashboard_data.get('dashboard_title', 'Unknown')
                    
                    # Only analyze selected dashboards
                    if dashboard_id not in selected_dashboard_ids:
                        logger.debug(f"â­ï¸ è·³è¿‡æœªé€‰ä¸­çš„Dashboard: {dashboard_title}")
                        return
                    
                    logger.info(f"ğŸ¤– åˆ†æDashboard {dashboard_index + 1}/{total_dashboards}: {dashboard_title}")
                    
                    # Analyze this dashboard immediately
                    analysis_result = await asyncio.get_event_loop().run_in_executor(
                        None, 
                        ai_analyzer.analyze_dashboard_progressively, 
                        question, 
                        dashboard_data
                    )
                    
                    # Store the analysis result
                    individual_analyses.append({
                        'dashboard_title': dashboard_title,
                        'analysis': analysis_result,
                        'dashboard_index': dashboard_index,
                        'timestamp': datetime.now().isoformat(),
                        'dashboard_url': dashboard_data.get('url', dashboard_data.get('dashboard_url', '')),
                        'dashboard_screenshot': dashboard_data.get('screenshot_path', ''),
                        'chart_analyses': dashboard_data.get('chart_analyses', [])
                    })
                    
                    logger.info(f"âœ… å®ŒæˆDashboardåˆ†æ: {dashboard_title}")
                    
                except Exception as e:
                    logger.error(f"âŒ Dashboardåˆ†æå¤±è´¥: {e}")
                    # Store error result
                    individual_analyses.append({
                        'dashboard_title': dashboard_data.get('dashboard_title', 'Unknown'),
                        'analysis': f"åˆ†ææ­¤çœ‹æ¿æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}",
                        'dashboard_index': dashboard_index,
                        'timestamp': datetime.now().isoformat(),
                        'error': True,
                        'dashboard_url': dashboard_data.get('url', dashboard_data.get('dashboard_url', '')),
                        'dashboard_screenshot': dashboard_data.get('screenshot_path', ''),
                        'chart_analyses': dashboard_data.get('chart_analyses', [])
                    })
            
            # Capture dashboards progressively (like test_download_all_dashboards.py)
            dashboards_data = await superset_automation.capture_dashboards_progressively(analyze_dashboard_callback)
            
            if not dashboards_data:
                # No dashboard data available
                print("âš ï¸  æ— æ³•è·å–çœ‹æ¿æ•°æ®")
                return {
                    'question': question,
                    'answer': 'æŠ±æ­‰ï¼Œç›®å‰æ— æ³•è¿æ¥åˆ°Supersetæˆ–è·å–çœ‹æ¿æ•°æ®ã€‚\n\nå¯èƒ½çš„åŸå› ï¼š\n1. SupersetæœåŠ¡æœªè¿è¡Œï¼ˆè¯·æ£€æŸ¥ http://localhost:8088ï¼‰\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. ç™»å½•å‡­æ®é”™è¯¯\n4. Superseté…ç½®é—®é¢˜\n\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n- ç¡®è®¤SupersetæœåŠ¡å·²å¯åŠ¨\n- æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®ï¼ˆ.envæ–‡ä»¶ï¼‰\n- éªŒè¯SUPERSET_URLã€SUPERSET_USERNAMEã€SUPERSET_PASSWORDæ˜¯å¦æ­£ç¡®\n- ç¨åé‡è¯•',
                    'timestamp': datetime.now().isoformat(),
                    'analysis_type': 'no_data',
                    'dashboards_analyzed': 0,
                    'total_charts': 0,
                    'dashboards_data': [],
                    'individual_analyses': [],
                    'screenshots': []
                }
            
            # Step 2: Prepare comprehensive analysis data
            print("ğŸ“Š Preparing comprehensive analysis data...")
            
            # Extract all screenshots for final summary
            all_screenshots = []
            chart_data_summary = []
            
            for dashboard in dashboards_data:
                # Add dashboard screenshot
                if dashboard.get('dashboard_screenshot'):
                    all_screenshots.append({
                        'title': f"{dashboard['dashboard_title']} - å®Œæ•´çœ‹æ¿",
                        'path': dashboard['dashboard_screenshot'],
                        'type': 'dashboard'
                    })
                
                # Add chart screenshots and data
                for chart in dashboard.get('charts', []):
                    if chart.get('chart_screenshot'):
                        all_screenshots.append({
                            'title': f"{chart['chart_title']} - å›¾è¡¨",
                            'path': chart['chart_screenshot'],
                            'type': 'chart'
                        })
                    
                    # Add chart data summary
                    chart_data_summary.append({
                        'dashboard': dashboard['dashboard_title'],
                        'chart': chart['chart_title'],
                        'data_type': chart.get('chart_data', {}).get('type', 'unknown'),
                        'key_metrics': _extract_key_metrics(chart.get('chart_data', {}))
                    })
            
            # Step 3: Combine all individual analyses
            print("ğŸ¤– Combining all individual analyses...")
            
            try:
                if individual_analyses and len(individual_analyses) > 1:
                    # Combine multiple analyses
                    final_answer = ai_analyzer.combine_multiple_analyses(question, individual_analyses)
                elif individual_analyses and len(individual_analyses) == 1:
                    # Only one dashboard, use the single analysis
                    final_answer = individual_analyses[0]['analysis']
                else:
                    # No individual analyses available, fallback to traditional method
                    dashboard_titles = [d['dashboard_title'] for d in dashboards_data]
                    if all_screenshots and any(os.path.exists(s['path']) for s in all_screenshots):
                        final_answer = ai_analyzer.analyze_with_screenshots(question, all_screenshots)
                    else:
                        enhanced_context = _create_enhanced_context(question, dashboards_data, chart_data_summary)
                        final_answer = ai_analyzer.analyze_text_only(enhanced_context, dashboard_titles)
                        
            except Exception as ai_error:
                print(f"AI analysis failed: {ai_error}")
                # Provide helpful error message with individual analyses if available
                if individual_analyses:
                    individual_analysis_text = "\n\nå„çœ‹æ¿çš„ç‹¬ç«‹åˆ†æï¼š\n"
                    for analysis in individual_analyses:
                        individual_analysis_text += f"\n--- {analysis['dashboard_title']} ---\n{analysis['analysis']}\n"
                    
                    return {
                        'question': question,
                        'answer': f'æŠ±æ­‰ï¼ŒAIç»¼åˆåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚ä½†ä»¥ä¸‹æ˜¯å„çœ‹æ¿çš„ç‹¬ç«‹åˆ†æï¼š{individual_analysis_text}\n\næŠ€æœ¯è¯¦æƒ…ï¼š{str(ai_error)}\n\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š\n- æ£€æŸ¥AI APIå¯†é’¥é…ç½®ï¼ˆOPENAI_API_KEYï¼‰\n- ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸\n- éªŒè¯APIæœåŠ¡çŠ¶æ€\n- ç¨åé‡è¯•',
                        'timestamp': datetime.now().isoformat(),
                        'analysis_type': 'ai_error_but_individual',
                        'dashboards_analyzed': len(dashboards_data),
                        'total_charts': sum(len(d.get('charts', [])) for d in dashboards_data),
                        'dashboards_data': dashboards_data,
                        'chart_summary': chart_data_summary,
                        'screenshots': all_screenshots,
                        'individual_analyses': individual_analyses
                    }
                else:
                    return {
                        'question': question,
                        'answer': f'æŠ±æ­‰ï¼ŒAIåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚æˆ‘ä»¬å·²ç»æˆåŠŸè·å–äº†çœ‹æ¿æ•°æ®ï¼Œä½†æ— æ³•è¿›è¡Œåˆ†æã€‚\n\næŠ€æœ¯è¯¦æƒ…ï¼š{str(ai_error)}\n\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š\n- æ£€æŸ¥AI APIå¯†é’¥é…ç½®ï¼ˆOPENAI_API_KEYï¼‰\n- ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸\n- éªŒè¯APIæœåŠ¡çŠ¶æ€\n- ç¨åé‡è¯•\n\nå·²è·å–çš„æ•°æ®ï¼š\n- åˆ†æäº† {len(dashboards_data)} ä¸ªçœ‹æ¿\n- åŒ…å« {sum(len(d.get("charts", [])) for d in dashboards_data)} ä¸ªå›¾è¡¨',
                        'timestamp': datetime.now().isoformat(),
                        'analysis_type': 'ai_error',
                        'dashboards_analyzed': len(dashboards_data),
                        'total_charts': sum(len(d.get('charts', [])) for d in dashboards_data),
                        'dashboards_data': dashboards_data,
                        'chart_summary': chart_data_summary,
                        'screenshots': all_screenshots,
                        'individual_analyses': individual_analyses
                    }
            
            return {
                'question': question,
                'answer': final_answer,
                'timestamp': datetime.now().isoformat(),
                'analysis_type': 'progressive',
                'dashboards_analyzed': len(dashboards_data),
                'total_charts': sum(len(d.get('charts', [])) for d in dashboards_data),
                'dashboards_data': dashboards_data,
                'chart_summary': chart_data_summary,
                'screenshots': all_screenshots,
                'individual_analyses': individual_analyses
            }
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            return {
                'question': question,
                'answer': f'æŠ±æ­‰ï¼Œåˆ†æè¿‡ç¨‹ä¸­é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚\n\né”™è¯¯è¯¦æƒ…ï¼š{str(e)}\n\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n- æ£€æŸ¥SupersetæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ\n- ç¡®è®¤ç½‘ç»œè¿æ¥ç¨³å®š\n- éªŒè¯æµè§ˆå™¨è‡ªåŠ¨åŒ–ç»„ä»¶æ˜¯å¦æ­£å¸¸\n- ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ\n\næŠ€æœ¯æ”¯æŒä¿¡æ¯ï¼š\n- è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼šapp.log, superset_automation.log\n- ç¡®è®¤æ‰€æœ‰ä¾èµ–é¡¹å·²æ­£ç¡®å®‰è£…',
                'timestamp': datetime.now().isoformat(),
                'analysis_type': 'error',
                'dashboards_analyzed': 0,
                'total_charts': 0,
                'dashboards_data': [],
                'chart_summary': [],
                'screenshots': [],
                'individual_analyses': []
            }

def _extract_key_metrics(chart_data):
    """Extract key metrics from chart data"""
    try:
        metrics = []
        data = chart_data.get('data', {})
        
        if 'total_sales' in data:
            metrics.append(f"æ€»é”€å”®é¢: {data['total_sales']:,}")
        if 'growth_rate' in data:
            metrics.append(f"å¢é•¿ç‡: {data['growth_rate']}%")
        if 'total_users' in data:
            metrics.append(f"æ€»ç”¨æˆ·æ•°: {data['total_users']:,}")
        if 'active_users' in data:
            metrics.append(f"æ´»è·ƒç”¨æˆ·: {data['active_users']:,}")
        if 'retention_rate' in data:
            metrics.append(f"ç•™å­˜ç‡: {data['retention_rate']}%")
        if 'value' in data:
            metrics.append(f"æ•°å€¼: {data['value']:,}")
        
        return metrics if metrics else ["æ•°æ®ä¸å¯ç”¨"]
    except:
        return ["æ•°æ®æå–å¤±è´¥"]

def _create_enhanced_context(question, dashboards_data, chart_summary):
    """Create enhanced context for AI analysis"""
    context = f"ä¸šåŠ¡é—®é¢˜: {question}\n\n"
    context += "å¯ç”¨çš„çœ‹æ¿å’Œå›¾è¡¨æ•°æ®:\n\n"
    
    for dashboard in dashboards_data:
        context += f"ğŸ“Š {dashboard['dashboard_title']}:\n"
        for chart in dashboard.get('charts', []):
            context += f"  ğŸ“ˆ {chart['chart_title']}\n"
            
            # Add key metrics
            metrics = _extract_key_metrics(chart.get('chart_data', {}))
            for metric in metrics:
                context += f"    â€¢ {metric}\n"
        context += "\n"
    
    context += "è¯·åŸºäºä»¥ä¸Šè¯¦ç»†çš„çœ‹æ¿å’Œå›¾è¡¨æ•°æ®åˆ†æä¸šåŠ¡é—®é¢˜ï¼Œæä¾›å…·ä½“çš„æ´å¯Ÿå’Œå»ºè®®ã€‚"
    
    return context

@app.route('/analyze', methods=['POST'])
def analyze():
    logger.info("ğŸ” æ”¶åˆ°åˆ†æè¯·æ±‚")
    try:
        data = request.get_json()
        question = data.get('question', '')
        
        logger.info(f"ğŸ“ ç”¨æˆ·é—®é¢˜: {question}")
        
        if not question:
            logger.warning("âš ï¸ ç”¨æˆ·é—®é¢˜ä¸ºç©º")
            return jsonify({'error': 'Question is required'}), 400
        
        logger.info("ğŸš€ å¼€å§‹å¼‚æ­¥åˆ†æ")
        # Run analysis in a separate thread to avoid blocking
        import concurrent.futures
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(run_async_analysis, question)
            result = future.result(timeout=120)  # 2 minute timeout
        
        logger.info(f"âœ… åˆ†æå®Œæˆï¼Œåˆ†æäº† {result.get('dashboards_analyzed', 0)} ä¸ªçœ‹æ¿")
        return jsonify(result)
    
    except concurrent.futures.TimeoutError:
        logger.error("â° åˆ†æè¶…æ—¶")
        return jsonify({
            'error': 'åˆ†æè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–ç®€åŒ–é—®é¢˜ã€‚\n\nå¯èƒ½çš„åŸå› ï¼š\n- ç½‘ç»œè¿æ¥ç¼“æ…¢\n- Supersetå“åº”æ—¶é—´è¿‡é•¿\n- çœ‹æ¿æ•°æ®é‡è¿‡å¤§\n- AIæœåŠ¡å“åº”å»¶è¿Ÿ\n\nå»ºè®®ï¼š\n- ç®€åŒ–åˆ†æé—®é¢˜\n- ç¨åé‡è¯•\n- æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€'
        }), 504
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¯·æ±‚å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/screenshots/<filename>')
def serve_screenshot(filename):
    """Serve screenshot files"""
    logger.info(f"ğŸ–¼ï¸ ç”¨æˆ·è¯·æ±‚æˆªå›¾: {filename}")
    try:
        # Check if file exists in screenshots directory
        screenshots_dir = os.path.join(os.path.dirname(__file__), 'screenshots')
        file_path = os.path.join(screenshots_dir, filename)
        
        if not os.path.exists(file_path):
            logger.error(f"âŒ æˆªå›¾æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return jsonify({'error': 'Screenshot not found', 'path': file_path}), 404
        
        logger.info(f"âœ… æ‰¾åˆ°æˆªå›¾æ–‡ä»¶: {file_path}")
        return send_from_directory(screenshots_dir, filename)
    except Exception as e:
        logger.error(f"âŒ æˆªå›¾æœåŠ¡å¤±è´¥ {filename}: {e}")
        return jsonify({'error': f'Screenshot service error: {str(e)}'}), 404

@app.route('/analyze_progressive', methods=['POST'])
def analyze_progressive():
    """Progressive analysis endpoint with real-time updates via SSE
    This implements the progressive analysis feature described in README.md:
    - Real-time chart analysis feedback
    - Server-Sent Events for live updates
    - Individual chart analysis with progress callbacks
    """
    logger.info("ğŸ” æ”¶åˆ°æ¸è¿›å¼åˆ†æè¯·æ±‚")
    try:
        data = request.get_json()
        question = data.get('question', '')
        
        logger.info(f"ğŸ“ ç”¨æˆ·é—®é¢˜: {question}")
        
        if not question:
            logger.warning("âš ï¸ ç”¨æˆ·é—®é¢˜ä¸ºç©º")
            return jsonify({'error': 'Question is required'}), 400
        
        logger.info("ğŸš€ å¼€å§‹æ¸è¿›å¼åˆ†æ")
        
        # Create a queue for this analysis session
        event_queue = Queue()
        
        # Start progressive analysis in a separate thread
        def run_progressive_analysis():
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    loop.run_until_complete(run_progressive_analysis_async(question, event_queue))
                finally:
                    loop.close()
                    
                # Send completion signal
                event_queue.put({
                    'type': 'complete',
                    'data': {'message': 'åˆ†æå®Œæˆ'}
                })
                
            except Exception as e:
                logger.error(f"âŒ æ¸è¿›å¼åˆ†æå¤±è´¥: {e}")
                
                # Check if it's a timeout error
                error_message = str(e)
                if "Timeout" in error_message or "timeout" in error_message:
                    error_message = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°è¶…æ—¶é”™è¯¯ã€‚å¯èƒ½çš„åŸå› ï¼š\n- æŸäº›çœ‹æ¿åŠ è½½æ—¶é—´è¿‡é•¿\n- ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n- SupersetæœåŠ¡å™¨å“åº”æ…¢\n\nå»ºè®®ï¼š\n- ç¨åé‡è¯•\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- è€ƒè™‘ç®€åŒ–åˆ†æé—®é¢˜\n- è”ç³»ç®¡ç†å‘˜ä¼˜åŒ–çœ‹æ¿æ€§èƒ½"
                else:
                    error_message = str(e)
                
                event_queue.put({
                    'type': 'error',
                    'data': {'error': error_message}
                })
        
        # Start the analysis thread
        analysis_thread = threading.Thread(target=run_progressive_analysis)
        analysis_thread.daemon = True
        analysis_thread.start()
        
        # Return SSE response
        def generate():
            yield f"data: {json.dumps({'type': 'start', 'message': 'å¼€å§‹åˆ†æ'})}\n\n"
            
            while True:
                try:
                    event = event_queue.get(timeout=300)  # 5 minute timeout for progressive analysis
                    yield f"data: {json.dumps(event)}\n\n"
                    
                    if event['type'] in ['complete', 'error']:
                        break
                        
                except Exception as e:
                    yield f"data: {json.dumps({'type': 'error', 'data': {'error': str(e)}})}\n\n"
                    break
        
        return Response(generate(), mimetype='text/event-stream')
    
    except Exception as e:
        logger.error(f"âŒ æ¸è¿›å¼åˆ†æè¯·æ±‚å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

async def run_progressive_analysis_async(question, event_queue):
    """Run progressive analysis with real-time updates using smart context system"""
    async with SupersetAutomation() as superset_automation:
        ai_analyzer = AIAnalyzer()
        smart_context = SmartContextSystem(ai_analyzer)
        
        try:
            # Send initial status
            event_queue.put({
                'type': 'status',
                'data': {'message': 'æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½åˆ†æç³»ç»Ÿ...', 'step': 'initializing'}
            })
            
            # Store individual analysis results
            individual_analyses = []
            dashboards_data = []
            
            # Step 1: Use smart context system to select top 3 relevant dashboards
            event_queue.put({
                'type': 'status',
                'data': {'message': 'æ­£åœ¨åˆ†æé—®é¢˜å¹¶é€‰æ‹©æœ€ç›¸å…³çš„çœ‹æ¿...', 'step': 'selecting_dashboards'}
            })
            
            logger.info(f"ğŸ§  Using smart context to select dashboards for question: {question}")
            
            # Select top 3 relevant dashboards based on question
            selected_dashboards_with_scores = smart_context.select_dashboards_for_question(question, top_k=3)
            
            # Convert to dict format for easier processing
            selected_dashboards = []
            for dashboard_context, score in selected_dashboards_with_scores:
                selected_dashboards.append({
                    'dashboard_id': dashboard_context.dashboard_id,
                    'dashboard_title': dashboard_context.dashboard_name,
                    'score': score
                })
            
            if not selected_dashboards:
                logger.warning("âš ï¸ No relevant dashboards found")
                event_queue.put({
                    'type': 'no_data',
                    'data': {
                        'question': question,
                        'answer': 'æŠ±æ­‰ï¼Œæ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„çœ‹æ¿æ•°æ®ã€‚\n\nå»ºè®®ï¼š\n- å°è¯•æ›´å…·ä½“çš„é—®é¢˜æè¿°\n- ä½¿ç”¨å…¶ä»–ç›¸å…³çš„å…³é”®è¯\n- æ£€æŸ¥çœ‹æ¿æ•°æ®æ˜¯å¦åŒ…å«æ‚¨éœ€è¦çš„ä¿¡æ¯'
                    }
                })
                return
            
            logger.info(f"âœ… Selected {len(selected_dashboards)} relevant dashboards")
            
            # Send selected dashboards info
            dashboard_titles = [d.get('dashboard_title', 'Unknown') for d in selected_dashboards]
            event_queue.put({
                'type': 'dashboards_selected',
                'data': {
                    'message': f'å·²é€‰æ‹© {len(selected_dashboards)} ä¸ªæœ€ç›¸å…³çš„çœ‹æ¿è¿›è¡Œåˆ†æ',
                    'dashboards': dashboard_titles
                }
            })
            
            # Get full dashboard list for processing
            full_dashboard_list = await superset_automation.get_dashboard_list()
            if not full_dashboard_list:
                logger.warning("âš ï¸ No dashboards available")
                event_queue.put({
                    'type': 'no_data',
                    'data': {
                        'question': question,
                        'answer': 'æŠ±æ­‰ï¼Œç›®å‰æ— æ³•è·å–çœ‹æ¿åˆ—è¡¨ã€‚\n\nå¯èƒ½çš„åŸå› ï¼š\n1. SupersetæœåŠ¡æœªè¿è¡Œ\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. ç™»å½•å‡­æ®é”™è¯¯\n\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n- ç¡®è®¤SupersetæœåŠ¡å·²å¯åŠ¨\n- æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®\n- ç¨åé‡è¯•'
                    }
                })
                return
            
            # Create a mapping of dashboard ID to full dashboard info
            dashboard_map = {str(d.get('id')): d for d in full_dashboard_list}
            
            # Process only the selected dashboards
            total_selected = len(selected_dashboards)
            
            # Define callback function for progressive analysis
            async def analyze_dashboard_callback(dashboard_data, dashboard_index, total_dashboards):
                """Callback function to analyze each dashboard immediately after capture"""
                try:
                    # Send status update
                    event_queue.put({
                        'type': 'dashboard_captured',
                        'data': {
                            'dashboard_title': dashboard_data.get('dashboard_title', 'Unknown'),
                            'dashboard_index': dashboard_index,
                            'total_dashboards': total_dashboards,
                            'dashboard_screenshot': dashboard_data.get('dashboard_screenshot'),
                            'charts': dashboard_data.get('charts', [])
                        }
                    })
                    
                    print(f"ğŸ¤– Analyzing dashboard {dashboard_index + 1}/{total_dashboards}: {dashboard_data.get('dashboard_title', 'Unknown')}")
                    
                    # Send analysis status
                    event_queue.put({
                        'type': 'analyzing',
                        'data': {
                            'dashboard_title': dashboard_data.get('dashboard_title', 'Unknown'),
                            'dashboard_index': dashboard_index,
                            'total_dashboards': total_dashboards
                        }
                    })
                    
                    # Send analysis start status
                    event_queue.put({
                        'type': 'analysis_started',
                        'data': {
                            'dashboard_title': dashboard_data.get('dashboard_title', 'Unknown'),
                            'dashboard_index': dashboard_index,
                            'total_dashboards': total_dashboards,
                            'message': f'å¼€å§‹åˆ†æçœ‹æ¿: {dashboard_data.get("dashboard_title", "Unknown")}',
                            'dashboard_screenshot': dashboard_data.get('dashboard_screenshot'),
                            'charts': dashboard_data.get('charts', [])
                        }
                    })
                    
                    # Define progress callback for chart analysis events
                    def progress_callback(event):
                        """Handle progress events from progressive analysis"""
                        try:
                            # Map the event types to SSE events
                            if event['type'] == 'chart_analysis_started':
                                event_queue.put({
                                    'type': 'chart_analysis_started',
                                    'data': {
                                        'dashboard_title': event['dashboard_title'],
                                        'chart_title': event['chart_title'],
                                        'chart_index': event['chart_index'],
                                        'total_charts': event['total_charts'],
                                        'chart_screenshot': event.get('chart_screenshot'),
                                        'message': event['message']
                                    }
                                })
                            elif event['type'] == 'chart_analysis_complete':
                                event_queue.put({
                                    'type': 'chart_analysis_complete',
                                    'data': {
                                        'dashboard_title': event['dashboard_title'],
                                        'chart_title': event['chart_title'],
                                        'chart_index': event['chart_index'],
                                        'total_charts': event['total_charts'],
                                        'chart_screenshot': event.get('chart_screenshot'),
                                        'analysis': event['analysis'],
                                        'message': event['message']
                                    }
                                })
                            elif event['type'] == 'dashboard_analysis_started':
                                event_queue.put({
                                    'type': 'dashboard_analysis_started',
                                    'data': {
                                        'dashboard_title': event['dashboard_title'],
                                        'message': event['message']
                                    }
                                })
                            elif event['type'] == 'dashboard_analysis_complete':
                                event_queue.put({
                                    'type': 'dashboard_analysis_complete',
                                    'data': {
                                        'dashboard_title': event['dashboard_title'],
                                        'analysis': event['analysis'],
                                        'message': event['message']
                                    }
                                })
                        except Exception as e:
                            print(f"âŒ Error in progress callback: {e}")
                    
                    # Analyze this dashboard with progress callback
                    analysis_result = await asyncio.get_event_loop().run_in_executor(
                        None, 
                        ai_analyzer.analyze_dashboard_progressively, 
                        question, 
                        dashboard_data,
                        progress_callback  # Pass the progress callback
                    )
                    
                    # Store the analysis result
                    analysis_entry = {
                        'dashboard_title': dashboard_data.get('dashboard_title', 'Unknown'),
                        'analysis': analysis_result,
                        'dashboard_index': dashboard_index,
                        'timestamp': datetime.now().isoformat(),
                        'dashboard_url': dashboard_data.get('url', dashboard_data.get('dashboard_url', '')),
                        'dashboard_screenshot': dashboard_data.get('dashboard_screenshot', ''),
                        'chart_analyses': dashboard_data.get('chart_analyses', [])
                    }
                    individual_analyses.append(analysis_entry)
                    
                    # Send final analysis result
                    event_queue.put({
                        'type': 'analysis_complete',
                        'data': analysis_entry
                    })
                    
                    print(f"âœ… Completed analysis for dashboard {dashboard_index + 1}/{total_dashboards}")
                    
                except Exception as e:
                    print(f"âŒ Error in dashboard analysis callback: {e}")
                    
                    # Check if it's a timeout error
                    error_message = str(e)
                    if "Timeout" in error_message or "timeout" in error_message:
                        error_message = f"çœ‹æ¿åŠ è½½è¶…æ—¶ï¼š{dashboard_data.get('dashboard_title', 'Unknown')}ã€‚å¯èƒ½çš„åŸå› ï¼š\n- çœ‹æ¿æ•°æ®é‡è¿‡å¤§\n- ç½‘ç»œè¿æ¥ç¼“æ…¢\n- SupersetæœåŠ¡å™¨å“åº”æ…¢\n\nå»ºè®®ï¼š\n- ç¨åé‡è¯•\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- è”ç³»ç®¡ç†å‘˜ä¼˜åŒ–çœ‹æ¿æ€§èƒ½"
                    else:
                        error_message = f"åˆ†ææ­¤çœ‹æ¿æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
                    
                    # Store error result
                    error_entry = {
                        'dashboard_title': dashboard_data.get('dashboard_title', 'Unknown'),
                        'analysis': error_message,
                        'dashboard_index': dashboard_index,
                        'timestamp': datetime.now().isoformat(),
                        'error': True,
                        'dashboard_url': dashboard_data.get('url', dashboard_data.get('dashboard_url', '')),
                        'dashboard_screenshot': dashboard_data.get('dashboard_screenshot', ''),
                        'chart_analyses': dashboard_data.get('chart_analyses', [])
                    }
                    individual_analyses.append(error_entry)
                    
                    # Send error result
                    event_queue.put({
                        'type': 'analysis_error',
                        'data': error_entry
                    })
            
            # Send status update
            event_queue.put({
                'type': 'status',
                'data': {'message': 'æ­£åœ¨è·å–çœ‹æ¿åˆ—è¡¨...', 'step': 'fetching_dashboards'}
            })
            
            # Process only the selected 3 dashboards
            event_queue.put({
                'type': 'status',
                'data': {'message': f'å¼€å§‹å¤„ç†é€‰ä¸­çš„ {len(selected_dashboards)} ä¸ªçœ‹æ¿...', 'step': 'processing_dashboards'}
            })
            
            # Process each selected dashboard individually
            for i, selected_dashboard in enumerate(selected_dashboards, 1):
                dashboard_id = selected_dashboard.get('dashboard_id')
                dashboard_title = selected_dashboard.get('dashboard_title', 'Unknown')
                
                logger.info(f"ğŸ”„ Processing selected dashboard {i}/{len(selected_dashboards)}: {dashboard_title}")
                
                # Get full dashboard info from the map
                full_dashboard_info = dashboard_map.get(dashboard_id)
                if not full_dashboard_info:
                    logger.warning(f"âš ï¸ Full dashboard info not found for {dashboard_id}")
                    continue
                
                # Capture dashboard screenshot using menu download
                event_queue.put({
                    'type': 'status',
                    'data': {'message': f'æ­£åœ¨ä¸‹è½½çœ‹æ¿æˆªå›¾: {dashboard_title}...', 'step': 'capturing_dashboard'}
                })
                
                try:
                    # Capture screenshot for this specific dashboard
                    screenshot_path = await superset_automation.capture_dashboard_screenshot(full_dashboard_info)
                    
                    if screenshot_path:
                        # Create dashboard data for analysis
                        dashboard_data = {
                            'dashboard_id': dashboard_id,
                            'dashboard_title': dashboard_title,
                            'dashboard_url': full_dashboard_info.get('url', ''),
                            'published': full_dashboard_info.get('published', False),
                            'changed_on': full_dashboard_info.get('changed_on', ''),
                            'dashboard_screenshot': get_screenshot_url(screenshot_path),
                            'charts': []  # Will be populated by AI analysis
                        }
                        
                        # Add to dashboards data list
                        dashboards_data.append(dashboard_data)
                        
                        # Call the analysis callback for this dashboard
                        await analyze_dashboard_callback(dashboard_data, i - 1, len(selected_dashboards))
                        
                    else:
                        logger.warning(f"âš ï¸ Failed to capture screenshot for {dashboard_title}")
                        event_queue.put({
                            'type': 'dashboard_capture_failed',
                            'data': {
                                'dashboard_title': dashboard_title,
                                'message': f'æ— æ³•ä¸‹è½½çœ‹æ¿æˆªå›¾: {dashboard_title}'
                            }
                        })
                        
                except Exception as e:
                    logger.error(f"âŒ Error processing dashboard {dashboard_title}: {e}")
                    event_queue.put({
                        'type': 'dashboard_capture_failed',
                        'data': {
                            'dashboard_title': dashboard_title,
                            'message': f'å¤„ç†çœ‹æ¿æ—¶å‡ºé”™: {dashboard_title} - {str(e)}'
                        }
                    })
            
            if not dashboards_data:
                # No dashboard data available
                event_queue.put({
                    'type': 'no_data',
                    'data': {
                        'question': question,
                        'answer': 'æŠ±æ­‰ï¼Œç›®å‰æ— æ³•è¿æ¥åˆ°Supersetæˆ–è·å–çœ‹æ¿æ•°æ®ã€‚\n\nå¯èƒ½çš„åŸå› ï¼š\n1. SupersetæœåŠ¡æœªè¿è¡Œï¼ˆè¯·æ£€æŸ¥ http://localhost:8088ï¼‰\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. ç™»å½•å‡­æ®é”™è¯¯\n4. Superseté…ç½®é—®é¢˜\n\nå»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n- ç¡®è®¤SupersetæœåŠ¡å·²å¯åŠ¨\n- æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®ï¼ˆ.envæ–‡ä»¶ï¼‰\n- éªŒè¯SUPERSET_URLã€SUPERSET_USERNAMEã€SUPERSET_PASSWORDæ˜¯å¦æ­£ç¡®\n- ç¨åé‡è¯•'
                    }
                })
                return
            
            # Step 2: Prepare comprehensive analysis data
            print("ğŸ“Š Preparing comprehensive analysis data...")
            
            # Extract all screenshots for final summary
            all_screenshots = []
            chart_data_summary = []
            
            for dashboard in dashboards_data:
                # Add dashboard screenshot
                if dashboard.get('dashboard_screenshot'):
                    all_screenshots.append({
                        'title': f"{dashboard['dashboard_title']} - å®Œæ•´çœ‹æ¿",
                        'path': dashboard['dashboard_screenshot'],
                        'type': 'dashboard'
                    })
                
                # Add chart screenshots and data
                for chart in dashboard.get('charts', []):
                    if chart.get('chart_screenshot'):
                        all_screenshots.append({
                            'title': f"{chart['chart_title']} - å›¾è¡¨",
                            'path': chart['chart_screenshot'],
                            'type': 'chart'
                        })
                    
                    # Add chart data summary
                    chart_data_summary.append({
                        'dashboard': dashboard['dashboard_title'],
                        'chart': chart['chart_title'],
                        'data_type': chart.get('chart_data', {}).get('type', 'unknown'),
                        'key_metrics': _extract_key_metrics(chart.get('chart_data', {}))
                    })
            
            # Step 3: Combine all individual analyses
            print("ğŸ¤– Combining all individual analyses...")
            
            # Send combining status
            event_queue.put({
                'type': 'combining',
                'data': {'message': 'æ­£åœ¨ç»¼åˆåˆ†æç»“æœ...'}
            })
            
            try:
                if individual_analyses and len(individual_analyses) > 1:
                    # Combine multiple analyses
                    final_answer = ai_analyzer.combine_multiple_analyses(question, individual_analyses)
                elif individual_analyses and len(individual_analyses) == 1:
                    # Only one dashboard, use the single analysis
                    final_answer = individual_analyses[0]['analysis']
                else:
                    # No individual analyses available, fallback to traditional method
                    dashboard_titles = [d['dashboard_title'] for d in dashboards_data]
                    if all_screenshots and any(os.path.exists(s['path']) for s in all_screenshots):
                        final_answer = ai_analyzer.analyze_with_screenshots(question, all_screenshots)
                    else:
                        enhanced_context = _create_enhanced_context(question, dashboards_data, chart_data_summary)
                        final_answer = ai_analyzer.analyze_text_only(enhanced_context, dashboard_titles)
                        
            except Exception as ai_error:
                print(f"AI analysis failed: {ai_error}")
                # Provide helpful error message with individual analyses if available
                if individual_analyses:
                    individual_analysis_text = "\n\nå„çœ‹æ¿çš„ç‹¬ç«‹åˆ†æï¼š\n"
                    for analysis in individual_analyses:
                        individual_analysis_text += f"\n--- {analysis['dashboard_title']} ---\n{analysis['analysis']}\n"
                    
                    final_answer = f'æŠ±æ­‰ï¼ŒAIç»¼åˆåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚ä½†ä»¥ä¸‹æ˜¯å„çœ‹æ¿çš„ç‹¬ç«‹åˆ†æï¼š{individual_analysis_text}\n\næŠ€æœ¯è¯¦æƒ…ï¼š{str(ai_error)}'
                else:
                    final_answer = f'æŠ±æ­‰ï¼ŒAIåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚æˆ‘ä»¬å·²ç»æˆåŠŸè·å–äº†çœ‹æ¿æ•°æ®ï¼Œä½†æ— æ³•è¿›è¡Œåˆ†æã€‚\n\næŠ€æœ¯è¯¦æƒ…ï¼š{str(ai_error)}'
            
            # Send final result
            event_queue.put({
                'type': 'final_result',
                'data': {
                    'question': question,
                    'answer': final_answer,
                    'timestamp': datetime.now().isoformat(),
                    'analysis_type': 'progressive',
                    'dashboards_analyzed': len(dashboards_data),
                    'total_charts': sum(len(d.get('charts', [])) for d in dashboards_data),
                    'dashboards_data': dashboards_data,
                    'chart_summary': chart_data_summary,
                    'screenshots': all_screenshots,
                    'individual_analyses': individual_analyses
                }
            })
            
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            event_queue.put({
                'type': 'error',
                'data': {
                    'question': question,
                    'answer': f'æŠ±æ­‰ï¼Œåˆ†æè¿‡ç¨‹ä¸­é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚\n\né”™è¯¯è¯¦æƒ…ï¼š{str(e)}',
                    'timestamp': datetime.now().isoformat(),
                    'analysis_type': 'error',
                    'dashboards_analyzed': 0,
                    'total_charts': 0,
                    'dashboards_data': [],
                    'chart_summary': [],
                    'screenshots': [],
                    'individual_analyses': []
                }
            })

@app.route('/health')
def health_check():
    """Health check endpoint"""
    logger.info("ğŸ’“ å¥åº·æ£€æŸ¥è¯·æ±‚")
    return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()})

def initialize_context_system():
    """Initialize and update context system on startup"""
    try:
        logger.info("ğŸ”§ åˆå§‹åŒ–æ™ºèƒ½Contextç³»ç»Ÿ...")
        
        # Initialize AI analyzer and context system
        ai_analyzer = AIAnalyzer()
        smart_context = SmartContextSystem(ai_analyzer)
        
        # Get system status
        status = smart_context.get_system_status()
        logger.info(f"ğŸ“Š Contextç³»ç»ŸçŠ¶æ€: {status}")
        
        # Check and update contexts
        logger.info("ğŸ”„ æ£€æŸ¥å¹¶æ›´æ–°Dashboard Contexts...")
        
        # Get available dashboards
        async def update_contexts():
            try:
                async with SupersetAutomation() as superset_automation:
                    # Step 1: Get dashboard list
                    logger.info("ğŸ“‹ Step 1: è·å–Dashboardåˆ—è¡¨...")
                    dashboard_list = await superset_automation.get_dashboard_list()
                    
                    if not dashboard_list:
                        logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„Dashboards")
                        return None
                    
                    logger.info(f"âœ… æ‰¾åˆ° {len(dashboard_list)} ä¸ªDashboards")
                    
                    # Step 2: Process each dashboard individually
                    logger.info("ğŸ”„ Step 2: å¾ªç¯æ£€æŸ¥æ¯ä¸ªDashboard...")
                    processed_count = 0
                    analysis_count = 0
                    all_update_results = {'updated_contexts': [], 'failed_updates': [], 'total_dashboards': len(dashboard_list)}
                    
                    for i, dashboard in enumerate(dashboard_list, 1):
                        dashboard_id = str(dashboard.get('id', ''))
                        dashboard_title = dashboard.get('title', '')
                        
                        logger.info(f"ğŸ” Step 2.{i}: æ£€æŸ¥Dashboard {dashboard_id}: {dashboard_title}")
                        
                        try:
                            # Step 2.1: Check if current dashboard context needs update
                            logger.info(f"ğŸ“ Step 2.{i}.1: æ£€æŸ¥Contextæ˜¯å¦éœ€è¦æ›´æ–°...")
                            
                            # Get existing context
                            existing_context = smart_context.context_manager.get_dashboard_context(dashboard_id)
                            
                            needs_update = False
                            if not existing_context:
                                logger.info(f"âš ï¸ Contextä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½å’Œåˆ†æ: {dashboard_title}")
                                needs_update = True
                            elif existing_context.is_expired(smart_context.context_manager.update_frequency_days):
                                logger.info(f"â° Contextå·²è¿‡æœŸï¼Œéœ€è¦æ›´æ–°: {dashboard_title}")
                                needs_update = True
                            else:
                                logger.info(f"âœ… Contextæœ‰æ•ˆï¼Œè·³è¿‡: {dashboard_title}")
                            
                            # Step 2.2: If needs update, download screenshot and analyze
                            if needs_update:
                                logger.info(f"ğŸ“¸ Step 2.{i}.2: ä¸‹è½½Dashboard Screenshot...")
                                
                                # Capture dashboard screenshot
                                screenshot_path = await superset_automation.capture_dashboard_screenshot(dashboard)
                                
                                if screenshot_path:
                                    logger.info(f"âœ… Screenshot captured: {screenshot_path}")
                                    
                                    # Create dashboard data
                                    dashboard_data = {
                                        'dashboard_id': dashboard_id,
                                        'dashboard_title': dashboard_title,
                                        'dashboard_url': dashboard.get('url', ''),
                                        'published': dashboard.get('published', False),
                                        'changed_on': dashboard.get('changed_on', ''),
                                        'dashboard_screenshot': get_screenshot_url(screenshot_path),
                                        'charts': []
                                    }
                                    
                                    # Step 2.2.1: Call AI analysis immediately
                                    logger.info(f"ğŸ¤– Step 2.{i}.2.1: è°ƒç”¨AIåˆ†æ...")
                                    
                                    # Update context for just this dashboard
                                    update_result = smart_context.update_dashboard_contexts([dashboard_data], force_update=True)
                                    
                                    if update_result and update_result.get('updated_contexts'):
                                        analysis_count += 1
                                        updated_context = update_result['updated_contexts'][0]
                                        all_update_results['updated_contexts'].append(updated_context)
                                        logger.info(f"âœ… AIåˆ†æå®Œæˆ: {dashboard_title} ({analysis_count}/{len(dashboard_list)})")
                                        
                                        # Verify context file was created
                                        context_file = f"context/{dashboard_id}.md"
                                        if os.path.exists(context_file):
                                            file_size = os.path.getsize(context_file)
                                            logger.info(f"ğŸ“ Contextæ–‡ä»¶å·²åˆ›å»º: {context_file} ({file_size} bytes)")
                                        else:
                                            logger.warning(f"âš ï¸ Contextæ–‡ä»¶æœªæ‰¾åˆ°: {context_file}")
                                    else:
                                        logger.warning(f"âš ï¸ AIåˆ†æå¤±è´¥: {dashboard_title}")
                                        all_update_results['failed_updates'].append({
                                            'dashboard_id': dashboard_id,
                                            'dashboard_name': dashboard_title,
                                            'error': 'AI analysis failed'
                                        })
                                else:
                                    logger.warning(f"âš ï¸ Screenshotä¸‹è½½å¤±è´¥: {dashboard_title}")
                                    all_update_results['failed_updates'].append({
                                        'dashboard_id': dashboard_id,
                                        'dashboard_name': dashboard_title,
                                        'error': 'Screenshot capture failed'
                                    })
                            else:
                                logger.info(f"âœ… Dashboardæ— éœ€æ›´æ–°: {dashboard_title}")
                            
                            processed_count += 1
                            
                        except Exception as e:
                            logger.error(f"âŒ å¤„ç†Dashboardå¤±è´¥ {dashboard_title}: {e}")
                            all_update_results['failed_updates'].append({
                                'dashboard_id': dashboard_id,
                                'dashboard_name': dashboard_title,
                                'error': str(e)
                            })
                        
                        # Add delay between processing to avoid overwhelming the system
                        if i < len(dashboard_list):
                            logger.info(f"â³ ç­‰å¾…3ç§’åå¤„ç†ä¸‹ä¸€ä¸ª...")
                            await asyncio.sleep(3)
                    
                    # Step 3: Rebuild FAISS index if contexts were updated
                    logger.info("ğŸ‰ Step 3: æ‰€æœ‰Dashboardå¤„ç†å®Œæˆï¼Œå¼€å§‹é‡å»ºFAISSç´¢å¼•...")
                    
                    if analysis_count > 0:
                        logger.info(f"ğŸ” æœ‰ {analysis_count} ä¸ªcontextè¢«æ›´æ–°ï¼Œé‡å»ºFAISSç´¢å¼•...")
                        if smart_context.use_faiss and smart_context.faiss_index_manager:
                            # Rebuild FAISS index with updated contexts
                            if smart_context.faiss_index_manager.build_index_from_contexts(force_rebuild=True):
                                logger.info("âœ… FAISSç´¢å¼•é‡å»ºæˆåŠŸ")
                            else:
                                logger.warning("âš ï¸ FAISSç´¢å¼•é‡å»ºå¤±è´¥")
                        else:
                            logger.warning("âš ï¸ FAISSæœªå¯ç”¨ï¼Œè·³è¿‡ç´¢å¼•é‡å»º")
                    else:
                        logger.info("â„¹ï¸ æ²¡æœ‰contextè¢«æ›´æ–°ï¼Œè·³è¿‡FAISSç´¢å¼•é‡å»º")
                    
                    # Step 4: Ready
                    logger.info("ğŸ‰ Step 4: ç³»ç»Ÿå®Œå…¨Ready!")
                    logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
                    logger.info(f"   - æ€»å…±Dashboards: {len(dashboard_list)}")
                    logger.info(f"   - å·²å¤„ç†: {processed_count}")
                    logger.info(f"   - AIåˆ†æå®Œæˆ: {analysis_count}")
                    logger.info(f"   - æˆåŠŸæ›´æ–°: {len(all_update_results.get('updated_contexts', []))}")
                    logger.info(f"   - å¤±è´¥: {len(all_update_results.get('failed_updates', []))}")
                    
                    # Log FAISS status
                    if smart_context.use_faiss and smart_context.faiss_index_manager:
                        faiss_status = smart_context.faiss_index_manager.get_index_status()
                        logger.info(f"ğŸ” FAISSçŠ¶æ€: {faiss_status['total_dashboards']} ä¸ªä»ªè¡¨æ¿å·²ç´¢å¼•")
                    
                    return all_update_results
                        
            except Exception as e:
                logger.error(f"âŒ Contextæ›´æ–°å¤±è´¥: {e}")
                return None
        
        # Run async update
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            update_results = loop.run_until_complete(update_contexts())
        finally:
            loop.close()
        
        logger.info("âœ… æ™ºèƒ½Contextç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info("ğŸš€ ç³»ç»Ÿå·²å°±ç»ªï¼Œç”¨æˆ·å¯ä»¥å¼€å§‹æé—®!")
        
        return update_results
        
    except Exception as e:
        logger.error(f"âŒ Contextç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return None

if __name__ == '__main__':
    logger.info("ğŸš€ å¯åŠ¨ Flask åº”ç”¨æœåŠ¡å™¨")
    
    # Initialize context system on startup
    try:
        update_results = initialize_context_system()
        if update_results is not None:
            logger.info("âœ… æ™ºèƒ½Contextç³»ç»Ÿå°±ç»ª")
        else:
            logger.warning("âš ï¸ æ™ºèƒ½Contextç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†ææ–¹æ³•")
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨æ—¶Contextæ£€æŸ¥å¤±è´¥: {e}")
        logger.warning("âš ï¸ å°†ç»§ç»­å¯åŠ¨ä½†ä¸ä½¿ç”¨æ™ºèƒ½Contextç³»ç»Ÿ")
    
    app.run(debug=True, host='0.0.0.0', port=5002)